{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fb6038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import math\n",
    "from scipy.optimize import minimize\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import vstack\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.optimize import curve_fit\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import poisson\n",
    "from scipy.special import erf\n",
    "from scipy.stats import gamma\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import time\n",
    "from scipy.optimize import newton\n",
    "from scipy.stats import norm\n",
    "from itertools import product\n",
    "import nbimporter\n",
    "import functions\n",
    "from tqdm import tqdm\n",
    "\n",
    "#nn modules\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch.utils.data import Dataset\n",
    "import networkx as nx\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import DenseGCNConv, global_mean_pool\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.utils import add_remaining_self_loops\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from graph_nets import blocks\n",
    "\n",
    "from graph_nets import graphs\n",
    "from graph_nets import modules\n",
    "from graph_nets import utils_np\n",
    "from graph_nets import utils_tf\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import sonnet as snt\n",
    "import tensorflow as tf\n",
    "\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch.utils.data import random_split\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.nn import Sequential, GCNConv\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch.nn.functional import mse_loss\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86585eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "electron_event_info = pd.read_csv('C:\\\\Users\\\\istok\\\\Downloads\\\\Electrons\\\\ele_event_cut_better.csv')\n",
    "electron_layer_info = pd.read_csv('C:\\\\Users\\\\istok\\\\Downloads\\\\Electrons\\\\ele_layer_cut_better.csv')\n",
    "\n",
    "electron_event_info = electron_event_info.astype('float32')\n",
    "\n",
    "# Convert all columns in pi0_layer_info to float32\n",
    "electron_layer_info = electron_layer_info.astype('float32')\n",
    "\n",
    "\n",
    "psb = electron_layer_info.iloc[:, 1:64] #9 eta * 7 phi\n",
    "emb1 = electron_layer_info.iloc[:, 64:65+50] #17 eta * 3 phi\n",
    "emb2 = electron_layer_info.iloc[:, 65+50:65+51+62] #9 eta * 7 phi\n",
    "emb3 = electron_layer_info.iloc[:, 65+51+62:65+51+62+63] #9 eta * 7 phi\n",
    "hab1 = electron_layer_info.iloc[:,65+51+62+63:] #9 eta * 7 phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376bbc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_and_restructure(df, df_name, num_segments, segment_size):\n",
    "    #Convert the DataFrame to a NumPy array for manipulation\n",
    "    data_array = df.to_numpy()\n",
    "    \n",
    "    #Reshape the array to 3D with dimensions: [num_rows, segment_size, num_segments]\n",
    "    #This groups each set of segments together for each row\n",
    "    reshaped_array = data_array.reshape(-1, num_segments, segment_size)\n",
    "    \n",
    "    #Sum over the last axis (num_segments) to combine the segments\n",
    "    summed_array = reshaped_array.sum(axis=1)\n",
    "    \n",
    "    #Convert the summed array back into a DataFrame with appropriate column names\n",
    "    column_names = [f\"{df_name} Et{i+1}\" for i in range(segment_size)]\n",
    "    summed_df = pd.DataFrame(summed_array, columns=column_names)\n",
    "    \n",
    "    return summed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefc0129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the function to each dataframe\n",
    "psb_Et = sum_and_restructure(psb, \"psb\", 7, 9)\n",
    "emb1_Et = sum_and_restructure(emb1, \"emb1\", 3, 17)\n",
    "emb2_Et = sum_and_restructure(emb2, \"emb2\", 7, 9)\n",
    "emb3_Et = sum_and_restructure(emb3, \"emb3\", 7, 9)\n",
    "hab1_Et = sum_and_restructure(hab1, \"hab1\", 7, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7b8050",
   "metadata": {},
   "outputs": [],
   "source": [
    "psb_eta = electron_event_info[\"psb eta\"]\n",
    "emb1_eta = electron_event_info[\"emb1 eta\"]\n",
    "emb2_eta = electron_event_info[\"emb2 eta\"]\n",
    "emb3_eta = electron_event_info[\"emb3 eta\"]\n",
    "hab1_eta = electron_event_info[\"hab1 eta\"]\n",
    "psb_emb1 = pd.DataFrame({\"psb - emb1\": (psb_eta - emb1_eta)})\n",
    "emb1_emb2 = pd.DataFrame({\"emb1 - emb2\": (emb1_eta - emb2_eta)})\n",
    "emb2_emb3 = pd.DataFrame({\"emb2 - emb3\": (emb2_eta - emb3_eta)})\n",
    "emb3_hab1 = pd.DataFrame({\"emb3 - hab1\": (emb3_eta - hab1_eta)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48efbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = electron_event_info[\"z\"].to_numpy()\n",
    "target = torch.tensor(labels, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e4d459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_tuple(graph_Etinfo, graph_etainfo, total_Etinfo):\n",
    "    graph_nx = nx.MultiDiGraph()\n",
    "    #nodes\n",
    "    for i in range(53):  # psb=9 l1=17 l2=9 l3=9 hab1=9\n",
    "        Et = graph_Etinfo[i]\n",
    "        eta = graph_etainfo[i]\n",
    "        total_Et = total_Etinfo[i]\n",
    "        graph_nx.add_node(i, features=np.array([Et, eta, total_Et]))\n",
    "    \n",
    "    graph_tuple = utils_np.networkxs_to_graphs_tuple([graph_nx])\n",
    "\n",
    "    return graph_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695f1b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Et_total = []\n",
    "for index, row in electron_layer_info.iterrows():\n",
    "    Et = row[1:].to_numpy()  \n",
    "    Et_total.append(np.sum(Et))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb599432",
   "metadata": {},
   "outputs": [],
   "source": [
    "Et_total_array = np.array(Et_total).reshape(-1, 1)\n",
    "\n",
    "#Initialize the MinMaxScaler to scale between 0 and 1\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "#Fit and transform the data to scale it\n",
    "Et_total_normalized = scaler.fit_transform(Et_total_array)\n",
    "\n",
    "#Flatten the array back to 1D\n",
    "Et_total_normalized = Et_total_normalized.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf75e0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Et_total_array.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf22a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "psb_Et_array = psb_Et.to_numpy()\n",
    "emb1_Et_array = emb1_Et.to_numpy()\n",
    "emb2_Et_array = emb2_Et.to_numpy()\n",
    "emb3_Et_array = emb3_Et.to_numpy()\n",
    "hab1_Et_array = hab1_Et.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a90e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_object(psb_Et_array, emb1_Et_array, emb2_Et_array, emb3_Et_array, hab1_Et_array,\n",
    "                 psb_eta, emb1_eta, emb2_eta, emb3_eta, hab1_eta,\n",
    "                 Et_total):\n",
    "    norm_psb = scaler.fit_transform(psb_Et_array.reshape(-1, 1))\n",
    "    norm_emb1 = scaler.fit_transform(emb1_Et_array.reshape(-1, 1))\n",
    "    norm_emb2 = scaler.fit_transform(emb2_Et_array.reshape(-1, 1))\n",
    "    norm_emb3 = scaler.fit_transform(emb3_Et_array.reshape(-1, 1))\n",
    "    norm_hab1 = scaler.fit_transform(hab1_Et_array.reshape(-1, 1))\n",
    "\n",
    "    graph_Etinfo = np.concatenate([norm_psb.flatten(), norm_emb1.flatten(),\n",
    "                                   norm_emb2.flatten(), norm_emb3.flatten(),\n",
    "                                   norm_hab1.flatten()])\n",
    "\n",
    "    graph_etainfo = np.concatenate([([psb_eta] * 9),\n",
    "                                    ([emb1_eta] * 17),\n",
    "                                    ([emb2_eta] * 9),\n",
    "                                    ([emb3_eta] * 9),\n",
    "                                    ([hab1_eta] * 9)])\n",
    "    \n",
    "    total_Etinfo = [Et_total] * 53\n",
    "    \n",
    "    return graph_tuple(graph_Etinfo, graph_etainfo, total_Etinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdf05df",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphlist = []\n",
    "for i in range(len(psb_Et)):\n",
    "    graph = create_object(psb_Et_array[i], emb1_Et_array[i], emb2_Et_array[i], emb3_Et_array[i], hab1_Et_array[i],\n",
    "                 psb_eta[i], emb1_eta[i], emb2_eta[i], emb3_eta[i], hab1_eta[i],\n",
    "                 Et_total_normalized[i])\n",
    "    \n",
    "    graphlist.append(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92709bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_objects = []\n",
    "for i in range(len(graphlist)):\n",
    "    node_features = graphlist[i].nodes\n",
    "    data = create_data_object(node_features)\n",
    "    data_objects.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8606fb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, graph, labels, globals_features):\n",
    "        self.graph = graph\n",
    "        self.labels = labels\n",
    "        self.globals = globals_features  #Store the global features\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graph)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        graph = self.graph[idx]\n",
    "        label = self.labels[idx]  #Retrieve the label corresponding to the current graph pair\n",
    "        global_features = self.globals[idx]  #Retrieve the global features corresponding to the current graph pair\n",
    "        return graph, label, global_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26662375",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_features = pd.concat([psb_emb1, emb1_emb2, emb2_emb3, emb3_hab1], axis = 1)\n",
    "global_features_np = global_features.to_numpy()\n",
    "\n",
    "global_features_tensor = torch.tensor(global_features_np, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd0c41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_dataset = GraphDataset(data_objects, target, global_features_tensor)\n",
    "total_size = len(paired_dataset)\n",
    "\n",
    "# Define the proportions\n",
    "train_size = int(0.7 * total_size)\n",
    "val_size = int(0.15 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset, test_dataset = random_split(paired_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c8a7b5",
   "metadata": {},
   "source": [
    "## GAT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be89da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(GAT, self).__init__()\n",
    "        \n",
    "        self.conv1 = GATv2Conv(in_channels=3, out_channels=6, heads=3, concat=True)\n",
    "        self.conv2 = GATv2Conv(in_channels=6*3, out_channels=9, heads=3, concat=True)\n",
    "        self.conv3 = GATv2Conv(in_channels=9*3, out_channels=3, heads=3, concat=False)\n",
    "        \n",
    "        self.fc1 = nn.Linear(63, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size * 2)  \n",
    "        self.fc3 = nn.Linear(hidden_size * 2, hidden_size * 3) \n",
    "        self.fc4 = nn.Linear(hidden_size * 3, hidden_size * 2)\n",
    "        self.fc5 = nn.Linear(hidden_size * 2, hidden_size) \n",
    "        self.fc6 = nn.Linear(hidden_size, 1)  \n",
    "\n",
    "\n",
    "    def forward(self, x, global_features, edge_index, batch, return_attention_weights=False):\n",
    "        attention_weights_list = []\n",
    "\n",
    "        x, attention_weights1 = self.conv1(x, edge_index, return_attention_weights=True)\n",
    "        attention_weights_list.append(attention_weights1)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x, attention_weights2 = self.conv2(x, edge_index, return_attention_weights=True)\n",
    "        attention_weights_list.append(attention_weights2)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x, attention_weights3 = self.conv3(x, edge_index, return_attention_weights=True)\n",
    "        attention_weights_list.append(attention_weights3)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x_reshaped = x.view(512, 53, 3)\n",
    "        \n",
    "        psb_eta, _= torch.max(x_reshaped[:,:9, 1], dim = 1)\n",
    "        emb1_eta, _ = torch.max(x_reshaped[:, 9:26, 1], dim = 1)\n",
    "        emb2_eta, _ = torch.max(x_reshaped[:,26:35, 1], dim = 1)\n",
    "        emb3_eta, _ = torch.max(x_reshaped[:,35:44, 1], dim = 1)\n",
    "        hab1_eta, _ = torch.max(x_reshaped[:,44:53, 1], dim = 1)\n",
    "        \n",
    "        total_Et = torch.mean(x_reshaped[:,:,2], dim=1)\n",
    "        \n",
    "        Et = x_reshaped[:, :, 0] \n",
    "\n",
    "        ini_features = torch.stack((psb_eta, emb1_eta, \n",
    "                                    emb2_eta, emb3_eta,\n",
    "                                    hab1_eta, total_Et), dim=1)  # Shape [512, 6]\n",
    "        \n",
    "        next_features = torch.cat((ini_features, global_features), dim=1) #shape [512, 10]\n",
    "        \n",
    "\n",
    "        final_features = torch.cat((next_features, Et), dim=1)  # Shape [512, 63]\n",
    "\n",
    "        x = F.relu(self.fc1(final_features))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "\n",
    "        if return_attention_weights:\n",
    "            return x, attention_weights_list\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c639ac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 53\n",
    "\n",
    "#Create a fully connected graph\n",
    "edges = [(i, j) for i in range(num_nodes) for j in range(num_nodes) if i != j]\n",
    "edges = torch.tensor(edges, dtype=torch.long).t().contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b578c47b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_fixed_eta = GAT(16)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model_fixed_eta.parameters(), lr=0.0005)\n",
    "\n",
    "loss_trend = []\n",
    "val_trend = []\n",
    "\n",
    "#example training loop\n",
    "for epoch in range(20):\n",
    "    model_fixed_eta.train()\n",
    "    train_loss = 0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        graph_data, labels, global_features = batch  \n",
    "        x, adj, batch_info = graph_data.x, graph_data.adj, graph_data.batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Forward pass\n",
    "        predictions = model_fixed_eta(x, global_features, edges, batch_info) \n",
    "        predictions = torch.squeeze(predictions)\n",
    "\n",
    "        #Compute loss\n",
    "        loss = loss_function(predictions, labels)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        #Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i == 0:  \n",
    "            print(f\"Epoch {epoch+1}, Batch 1, First Prediction: {predictions[0].item()}, First Label: {labels[0].item()}\")\n",
    "        \n",
    "    model_fixed_eta.eval()  \n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            graph_data, labels, global_features = batch  \n",
    "            x, adj, batch_info = graph_data.x, graph_data.adj, graph_data.batch\n",
    "            predictions = model_fixed_eta(x, global_features, edges, batch_info) \n",
    "            predictions = torch.squeeze(predictions)\n",
    "            loss = loss_function(predictions, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    loss_trend.append(avg_train_loss)\n",
    "    val_trend.append(avg_val_loss)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model_fixed_eta.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()\n",
    "            # Include any other stuff you need to resume training\n",
    "        }\n",
    "    torch.save(checkpoint, f'checkpoint_epoch_{epoch}.pth')\n",
    "    print(f\"Checkpoint saved at epoch {epoch}\")\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415f420c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
